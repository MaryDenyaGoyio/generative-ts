# train_gp_ls4.py
import os
import yaml
import torch
from torch.utils.data import DataLoader
import types
import numpy as np

class AttrDict(dict):

    def __getattr__(self, k):
        try:    return self[k]
        except KeyError:
            raise AttributeError(f"No attribute '{k}'")

    def __setattr__(self, k, v):
        self[k] = v

def dict2attr(d):

    if isinstance(d, dict):
        return AttrDict({k: dict2attr(v) for k, v in d.items()})
    elif isinstance(d, list):
        return [dict2attr(v) for v in d]
    else:   return d






# ============ LS4 DEBUG ============

import ls4.models.s4 as _s4
from ls4.models.s4 import SSKernelNPLR, S4


# ------ debug forward ------

_orig_ss_fwd = SSKernelNPLR.forward

def debug_ss_fwd(self, state=None, rate=1.0, L=None):
    print(
        f"[SSK.debug] L={L}, rate="
        f"{'tensor'+str(tuple(rate.shape)) if torch.is_tensor(rate) else rate}, "
        f"state={'None' if state is None else tuple(state.shape)}"
    )

    k, k_state = _orig_ss_fwd(self, state=state, rate=rate, L=L)

    print(
        f"[SSK.debug] â†’ k NaN? {k.isnan().any().item()}, "
        f"k_state NaN? { (k_state.isnan().any().item() if torch.is_tensor(k_state) else False) }"
    )
    return k, k_state

SSKernelNPLR.forward = debug_ss_fwd


# ------ debug cauchy ------

_orig_cauchy_conj = getattr(_s4, 'cauchy_conj', None)
if _orig_cauchy_conj is not None:
    def cauchy_conj_stable(v, z, w, eps: float = 1e-6):
        # ë³µì†Œ ì•ˆì •í™”: ë¶„ëª¨ê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒì„ í”¼í•˜ê¸° ìœ„í•´ zì— ì‘ì€ í—ˆìˆ˜ ì„±ë¶„ ì¶”ê°€
        if z.dtype == torch.cfloat:
            z = z + 1j * eps
        elif z.dtype == torch.cdouble:
            z = z + 1j * (eps)

        # ê³ ì •ë°€ ê³„ì‚° í›„ ë°˜í™˜ì„ ì…ë ¥ ì •ë°€ë„ë¡œ ë³µê·€
        use_double = (v.dtype == torch.cfloat)
        if use_double:
            v_d = v.to(torch.cdouble)
            z_d = z.to(torch.cdouble)
            w_d = w.to(torch.cdouble)
            r = _orig_cauchy_conj(v_d, z_d, w_d)
            return r.to(torch.cfloat)
        else:
            return _orig_cauchy_conj(v, z, w)

    _s4.cauchy_conj = cauchy_conj_stable


# ------ debug nan ------

def nan_logger_hook(module, inp, out):
    def check(t: torch.Tensor, name: str):
        # NaN ì´ ì•„ë‹Œ ë¶€ë¶„ë§Œ ë½‘ì•„ì„œ min/max ê³„ì‚°
        mask = ~torch.isnan(t)
        if mask.any():
            t_valid = t[mask]
            t_min, t_max = t_valid.min().item(), t_valid.max().item()
        else:
            t_min, t_max = float('nan'), float('nan')
        if torch.isnan(t).any():
            print(f"[NaN] {module.__class__.__name__}.{name} ì— NaN ë°œê²¬! valid min/max:", t_min, t_max)
    if isinstance(out, torch.Tensor):
        check(out, "output")
    elif isinstance(out, (tuple, list)):
        for i, o in enumerate(out):
            if torch.is_tensor(o):
                check(o, f"output[{i}]")









# ============ LS4 override ============

from ls4.models.ls4 import VAE as _BaseVAE

class VAE(_BaseVAE):
    """
    ì›ë³¸ VAEì—ì„œ decoder ì¶œë ¥(dec_mean, dec_std)ë§Œ ê³ ì •ëœ p(x|z)=N(z, Ïƒ^2) í˜•íƒœë¡œ ë®ì–´ì”Œì›ë‹ˆë‹¤.
    encoder, prior, kld, nll ê³„ì‚° ë“± ë‚˜ë¨¸ì§€ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    def forward(self, x, timepoints, masks, labels=None, plot=False, sum=False):
        # 1) posterior sample
        z_post, z_post_mean, z_post_std = self.encoder.encode(x, timepoints, use_forward=True)

        # (bidirectional ì—¬ë¶€ì— ë”°ë¼ ë’¤ìª½ë„ ì²˜ë¦¬â€”but ì—¬ê¸°ì„  ìƒëµ)
        # 2) prior ê³„ì‚°ë§Œ ì›ë³¸ëŒ€ë¡œ ë°›ê³ 
        if self.bidirectional:
            # bidirectional ì¼ ë•Œ prior ê³„ì‚°
            dec_stats = self.decoder(x, timepoints, z_post, *self.encoder.encode(x, timepoints, use_forward=False))
            # dec_stats = (dec_mean, dec_std, z_prior_mean, z_prior_std, z_prior_mean_back, z_prior_std_back)
        else:
            # ë‹¨ë°©í–¥ì¼ ë•Œ prior ê³„ì‚°
            # ì›ë˜ self.decoder(x, timepoints, z_post) ì´ ë°˜í™˜í•˜ëŠ” íŠœí”Œì„ ë°›ë˜
            *_, z_prior_mean, z_prior_std = self.decoder(x, timepoints, z_post)

        # 3) ì˜¤ë¦¬ì§€ë„ ë””ì½”ë” mean/std ëŒ€ì‹  ë‚´ê°€ ê³ ì •í•œ p(x|z)=N(z_post, Ïƒ^2)
        Ïƒ = self.config.sigma
        dec_mean = z_post
        dec_std  = torch.ones_like(dec_mean) * Ïƒ

        # 4) ë‚˜ë¨¸ì§€ ELBO ì¡°ë¦½ì€ ì›ë³¸ëŒ€ë¡œ
        #    kld_loss ê³„ì‚°
        if self.bidirectional:
            # ë’¤ìª½ prior mean/std ì€ dec_stats ì—ì„œ ê°€ì ¸ì™€ì•¼ í•˜ì§€ë§Œâ€¦
            # (bidirectional ìƒí™©ì— ë§ê²Œ z_prior_mean_back, z_prior_std_back ë„£ì–´ì£¼ì„¸ìš”)
            z_prior_mean_back, z_prior_std_back = dec_stats[-2], dec_stats[-1]
            kld_f = self._kld_gauss(z_post_mean, z_post_std, z_prior_mean, z_prior_std, masks, sum=sum)
            kld_b = self._kld_gauss(*self.encoder.encode(x, timepoints, use_forward=False)[1:], 
                                     z_prior_mean_back, z_prior_std_back, masks, sum=sum)
            kld_loss = kld_f + kld_b
        else:
            kld_loss = self._kld_gauss(z_post_mean, z_post_std, z_prior_mean, z_prior_std, masks, sum=sum)

        #    nll_loss ê³„ì‚°
        nll_loss = self._nll_gauss(dec_mean, dec_std, x, masks, sum=sum)

        #    total ELBO
        loss = (kld_loss + nll_loss).mean()

        #    (ì´í›„ classification, logging ë“±ì€ ê·¸ëŒ€ë¡œ ë³µì‚¬)
        log_info = {
            'dec_mean': dec_mean.mean().item(),
            'kld_loss': kld_loss.mean().item(),
            'nll_loss': nll_loss.mean().item(),
            'loss': loss.item(),
        }
        return loss, log_info



EPS = torch.finfo(torch.float).eps

def patched_kld_gauss(self, mean_1, std_1, mean_2, std_2, masks=None, sum=False):
    # ---- debug print ì‹œì‘ ----
    if torch.any(std_1 <= 0) or torch.any(std_2 <= 0):
        print("âš ï¸ [kld] std_1.min/max:", std_1.min().item(), std_1.max().item())
        print("âš ï¸ [kld] std_2.min/max:", std_2.min().item(), std_2.max().item())
    # ---- ì›ë³¸ ê³„ì‚° ----
    kld_elem = (
        2 * torch.log(std_2 + EPS)
      - 2 * torch.log(std_1 + EPS)
      + (std_1.pow(2) + (mean_1 - mean_2).pow(2)) / std_2.pow(2)
      - 1
    )  # (B, L, D)
    # ---- debug nan ì²´í¬ ----
    if torch.isnan(kld_elem).any():
        print("ğŸ” nan in kld_elem!")
        # í•œ ë°°ì¹˜, í•œ ì‹œí€€ìŠ¤ ìƒ˜í”Œë§Œ ë³´ì—¬ì¤ë‹ˆë‹¤ (detach() ì¶”ê°€)
        print(" mean_1[0,0,:5]:", mean_1[0,0,:5].detach().cpu().numpy())
        print(" std_1[0,0,:5]: ", std_1[0,0,:5].detach().cpu().numpy())
        print(" mean_2[0,0,:5]:", mean_2[0,0,:5].detach().cpu().numpy())
        print(" std_2[0,0,:5]: ", std_2[0,0,:5].detach().cpu().numpy())
    # ---- aggregated return ----
    if masks is None:
        flat = kld_elem.view(kld_elem.size(0), -1)
        return 0.5 * (flat.sum(dim=1) if sum else flat.mean(dim=1))
    m = masks.unsqueeze(-1)  # (B, L, 1)
    if sum:
        return 0.5 * (kld_elem * m).sum(dim=1).sum(dim=1)
    else:
        lens = masks.sum(dim=1).clamp(min=1)            # (B,)
        per_t = (kld_elem * m).sum(dim=2)               # (B, L)
        return 0.5 * (per_t.sum(dim=1) / lens)          # (B,)

def patched_nll_gauss(self, mean, std, x, masks=None, sum=False):
    # ---- debug print ì‹œì‘ ----
    if torch.any(std <= 0):
        print("âš ï¸ [nll] std.min/max:", std.min().item(), std.max().item())
    # ---- ì›ë³¸ ê³„ì‚° ----
    nll = (
        torch.log(std + EPS)
      + np.log(2*np.pi)/2
      + (x - mean).pow(2) / (2 * std.pow(2))
    )  # (B, L, D)
    # ---- debug nan ì²´í¬ ----
    if torch.isnan(nll).any():
        print("ğŸ” nan in nll!")
        print(" mean[0,0]:", mean[0,0,0].detach().cpu().item())
        print(" std[0,0]: ", std[0,0,0].detach().cpu().item())
        print(" x[0,0]:   ",   x[0,0,0].detach().cpu().item())
    # ---- aggregated return ----
    if masks is None:
        flat = nll.view(nll.size(0), -1)
        return flat.sum(dim=1) if sum else flat.mean(dim=1)
    m = masks.unsqueeze(-1)
    if sum:
        return (nll * m).sum(dim=1).sum(dim=1)
    else:
        lens = masks.sum(dim=1).clamp(min=1)
        per_t = (nll * m).sum(dim=2)  # (B, L)
        return per_t.sum(dim=1) / lens

def patched_mse(self, mean, x, masks=None, sum=False):
    err = (x - mean).pow(2)  # (B, L, D)
    if torch.isnan(err).any():
        print("ğŸ” nan in mse err!")
    if masks is None:
        flat = err.view(err.size(0), -1)
        return flat.sum(dim=1) if sum else flat.mean(dim=1)
    m = masks.unsqueeze(-1)
    if sum:
        return (err * m).sum(dim=1).sum(dim=1)
    else:
        lens = masks.sum(dim=1).clamp(min=1)
        per_t = (err * m).sum(dim=2)  # (B, L)
        return per_t.sum(dim=1) / lens

# ë©”ì„œë“œ êµì²´
VAE._kld_gauss = types.MethodType(patched_kld_gauss, VAE)
VAE._nll_gauss = types.MethodType(patched_nll_gauss, VAE)
VAE._mse      = types.MethodType(patched_mse, VAE)



# 3) ëª¨ë¸Â·ë°ì´í„°ì…‹ import
from generative_ts.datasets.gp_dataset import GPSyntheticDataset

def main():
    device     = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    n_epochs   = 100
    batch_size = 64
    lr         = 1e-3

    # GP ë°ì´í„° íŒŒë¼ë¯¸í„°
    T, std_Y, v, tau, sigma_f = 500, 1.0, 10.0, 1.0, 1.0
    timepoints = torch.linspace(0.0, 1.0, T, device=device)
    num_seq = 2000
    dataset = GPSyntheticDataset(T, std_Y, v, tau, sigma_f, num_sequences=num_seq)
    loader  = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

    # YAML ì§ì ‘ ë¡œë“œ
    cfg_all = yaml.safe_load(open(
        "ls4/configs/monash/vae_nn5daily.yaml", "r", encoding="utf-8"
    ))
    cfg_all = dict2attr(cfg_all)
    model_cfg = cfg_all.model

    # **í•„ìˆ˜ ì„¤ì • ë³´ì¶©**
    model_cfg.n_labels   = 1      # GPì—” ë ˆì´ë¸” ì—†ìœ¼ë‹ˆ 1
    model_cfg.classifier = False  # ë¶„ë¥˜ê¸° ì‚¬ìš© ì•ˆ í•¨

    # ëª¨ë¸ ìƒì„±
    model     = VAE(model_cfg).to(device)
    model.setup_rnn(mode='dense')
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    for name, module in model.encoder.named_modules():
        if isinstance(module, SSKernelNPLR):
            module.register_forward_hook(
                lambda mod, inp, out, name=name:
                print(f"[HOOK] {name} (SSK) â†’ k NaN? {out[0].isnan().any().item()}")
            )
        if isinstance(module, S4):
            def _safe_nan_report(mod, inp, out, name=name):
                def tensor_has_nan(t: torch.Tensor) -> bool:
                    return bool(torch.isnan(t).any().item())
                flag = None
                if torch.is_tensor(out):
                    flag = tensor_has_nan(out)
                elif isinstance(out, (tuple, list)) and len(out) > 0 and torch.is_tensor(out[0]):
                    flag = tensor_has_nan(out[0])
                else:
                    flag = False
                print(f"[HOOK] {name} (S4) â†’ output NaN? {flag}")

            module.register_forward_hook(_safe_nan_report)

    # í•™ìŠµ ë£¨í”„
    print("================================================")
    print("ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜:", sum(p.numel() for p in model.parameters()))
    print("================================================")
    
    for epoch in range(1, n_epochs+1):
        model.train()
        total_loss = 0.0
        for i, x in enumerate(loader):
            x     = x.to(device).unsqueeze(-1)  # (B, T, 1)
            masks = torch.ones(x.shape[0], x.shape[1], device=device)

            optimizer.zero_grad()
            loss, log_info = model(x, timepoints, masks, plot=False, sum=True)
            loss.backward()
            optimizer.step()

            if i % 10 == 0:  
                print(f"{i}th : ")

                print(loss.item(), end=", ")
                for k,v in log_info.items():
                    # ìŠ¤ì¹¼ë¼ ê°’ë§Œ ì¶œë ¥
                    try:
                        print(f"  {k}:", float(v) if torch.is_tensor(v) else v)
                    except:
                        print(f"  {k}: (unprintable)")

            total_loss += loss.item() * x.size(0)

        avg_loss = total_loss / len(dataset)
        print(f"[Epoch {epoch:03d}] avg ELBO = {avg_loss:.4f}")


        os.makedirs("trained_models", exist_ok=True)
        torch.save(model.state_dict(), "trained_models/ls4_gp.pth")
        print("âœ… ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ 'trained_models/ls4_gp.pth'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()

